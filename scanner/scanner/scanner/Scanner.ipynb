{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba619959-cdf7-4275-9dbd-b53cc6312562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KEYWORD', 'int', 2, 0)\n",
      "('ID', 'main', 2, 4)\n",
      "('LPAREN', '(', 2, 8)\n",
      "('RPAREN', ')', 2, 9)\n",
      "('LBRACE', '{', 2, 11)\n",
      "('KEYWORD', 'int', 3, 4)\n",
      "('ID', 'x', 3, 8)\n",
      "('ASSIGN', '=', 3, 10)\n",
      "('NUMBER', 10, 3, 12)\n",
      "('END', ';', 3, 14)\n",
      "('KEYWORD', 'return', 4, 4)\n",
      "('ID', 'x', 4, 11)\n",
      "('OP', '+', 4, 13)\n",
      "('NUMBER', 5, 4, 15)\n",
      "('END', ';', 4, 16)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "'@' unexpected on line 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 69\u001b[0m\n\u001b[0;32m     60\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124mint main() \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124m    int x = 10;\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Call the lexical analyzer and print the tokens\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 54\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(code)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Raise an error for unexpected characters\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMISMATCH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m unexpected on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Yield the token type, value, line number, and column position\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m kind, value, line_num, column\n",
      "\u001b[1;31mRuntimeError\u001b[0m: '@' unexpected on line 5"
     ]
    }
   ],
   "source": [
    "import re  # Import the regular expression module\n",
    "\n",
    "# Define a list of keywords\n",
    "keywords = {'int', 'return', 'void', 'if', 'else', 'while', 'for', 'break', 'continue', 'switch', 'case'}\n",
    "\n",
    "# Define a list of token specifications using regular expressions\n",
    "token_specification = [\n",
    "    ('NUMBER', r'\\d+(\\.\\d*)?'),   # Numbers (integer or float)\n",
    "    ('ASSIGN', r'='),             # Assignment operator\n",
    "    ('END', r';'),                # Statement terminator\n",
    "    ('ID', r'[A-Za-z_][A-Za-z_0-9]*'),      # Identifier (variable or function name)\n",
    "    ('OP', r'[+\\-*/%]'),           # Arithmetic operators\n",
    "    ('LPAREN', r'\\('),            # Left parenthesis\n",
    "    ('RPAREN', r'\\)'),            # Right parenthesis\n",
    "    ('LBRACE', r'\\{'),            # Left brace\n",
    "    ('RBRACE', r'\\}'),            # Right brace\n",
    "    ('NEWLINE', r'\\n'),           # New line\n",
    "    ('SKIP', r'[ \\t]+'),          # Skip whitespace\n",
    "    ('COMPARISON', r'==|!=|<=|>=|<|>'),  # Comparison operators\n",
    "    ('MISMATCH', r'.'),           # Any unknown character\n",
    "]\n",
    "\n",
    "# Create a regex pattern from the token specifications\n",
    "tok_regex = '|'.join(f'(?P<{pair[0]}>{pair[1]})' for pair in token_specification)\n",
    "\n",
    "# Define the lexical analyzer function\n",
    "def tokenize(code):\n",
    "    # Initialize line number and line start position\n",
    "    line_num = 1\n",
    "    line_start = 0\n",
    "    \n",
    "    # Match the code against the patterns\n",
    "    for mo in re.finditer(tok_regex, code):\n",
    "        kind = mo.lastgroup  # Get the last matched group name (token type)\n",
    "        value = mo.group(kind)  # Get the matched value for that token\n",
    "        column = mo.start() - line_start  # Calculate the column position of the token\n",
    "\n",
    "        # Convert matched number strings to integer or float\n",
    "        if kind == 'NUMBER':\n",
    "            value = float(value) if '.' in value else int(value)\n",
    "        # Check if the identifier is a keyword\n",
    "        elif kind == 'ID' and value in keywords:\n",
    "            kind = 'KEYWORD'\n",
    "        # Handle new lines\n",
    "        elif kind == 'NEWLINE':\n",
    "            line_start = mo.end()  # Update line start to the end of the current match\n",
    "            line_num += 1  # Increment line number\n",
    "            continue  # Skip to the next match\n",
    "        # Ignore whitespace\n",
    "        elif kind == 'SKIP':\n",
    "            continue\n",
    "        # Raise an error for unexpected characters\n",
    "        elif kind == 'MISMATCH':\n",
    "            raise RuntimeError(f'{value!r} unexpected on line {line_num}')\n",
    "        \n",
    "        # Yield the token type, value, line number, and column position\n",
    "        yield kind, value, line_num, column\n",
    "\n",
    "# Example usage\n",
    "code = '''\n",
    "int main() {\n",
    "    int x = 10;\n",
    "    return x + 5;\n",
    "    @\n",
    "}\n",
    "'''\n",
    "\n",
    "# Call the lexical analyzer and print the tokens\n",
    "for token in tokenize(code):\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a822336-0e66-4b79-a225-e0c0629d8594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
